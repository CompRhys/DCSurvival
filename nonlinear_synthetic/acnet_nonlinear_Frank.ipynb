{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "seed = 142857\n",
    "rng = np.random.default_rng(seed)\n",
    "from statsmodels.distributions.copula.api import (\n",
    "    CopulaDistribution, GumbelCopula, FrankCopula, ClaytonCopula)\n",
    "\n",
    "torch.backends.cudnn.allow_tf32 = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "# Generate synthetic data (time-to-event and censoring indicator)\n",
    "# np.random.seed(285714)\n",
    "v_e=4; rho_e=17; v_c=3; rho_c=16\n",
    "\n",
    "# for non-linear dim=1\n",
    "size = 20000\n",
    "dim = 1\n",
    "\n",
    "# generate X from 10 dimensional uniform distribution from 0 to 1\n",
    "X = np.random.uniform(0, 1, (size, dim))\n",
    "\n",
    "# multiply beta_e with X to get event risk\n",
    "event_risk = 2* np.sin(X*np.pi).squeeze() \n",
    "# multiple beta_c with X to get censoring risk\n",
    "censoring_risk = 2* np.sin(X*np.pi+0.1).squeeze()\n",
    "\n",
    "copula = FrankCopula(theta=10)\n",
    "sample = copula.rvs(size)\n",
    "u = sample[:, 0]\n",
    "v = sample[:, 1]\n",
    "plt.scatter(u,v)\n",
    "\n",
    "\n",
    "# Generate according to Algorithm 2 in \"Copula-based Deep Survival Models for Dependent Censoring\"\n",
    "def inverse_transform(value, risk, shape, scale):\n",
    "    return (-np.log(value)/np.exp(risk))**(1/shape)*scale       \n",
    "    # return (-np.log(1-value)/np.exp(risk))**(1/shape)*scale\n",
    "\n",
    "\n",
    "event_time = inverse_transform(u, event_risk, v_e, rho_e)\n",
    "censoring_time = inverse_transform(v, censoring_risk, v_c, rho_c)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(event_time, censoring_time)\n",
    "\n",
    "# check censoring rate \n",
    "print(np.sum(event_time<censoring_time)/len(event_time))\n",
    "\n",
    "# create observed time \n",
    "observed_time = np.minimum(event_time, censoring_time)\n",
    "event_indicator = (event_time<censoring_time).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "times_tensor = torch.tensor(observed_time, dtype=torch.float64).to(device)\n",
    "event_indicator_tensor = torch.tensor(event_indicator, dtype=torch.float64).to(device)\n",
    "covariate_tensor = torch.tensor(X, dtype=torch.float64).to(device)\n",
    "train_data = TensorDataset(covariate_tensor[0:10000], times_tensor[0:10000], event_indicator_tensor[0:10000])\n",
    "val_data = TensorDataset(covariate_tensor[10000:], times_tensor[10000:], event_indicator_tensor[10000:])\n",
    "\n",
    "batch_size = 10000\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data, batch_size= batch_size, shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_data, batch_size= batch_size, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Likelihood with ACNET copula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function, gradcheck\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os,sys\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "from main import sample\n",
    "\n",
    "num_epochs = 500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dirac_phi import DiracPhi\n",
    "from survival import SurvivalCopula_nonlinear\n",
    "from survival import sample\n",
    "\n",
    "import wandb\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "run = wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"ACNet Survival\",\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"dataset\": \"Frank Copula Weibull Non-linear Hazzards\",\n",
    "    \"epochs\": 50000,\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# Early stopping\n",
    "best_val_loglikelihood = float('-inf')\n",
    "epochs_no_improve = 0\n",
    "early_stop_epochs = 5000\n",
    "\n",
    "# Parameters for ACNet\n",
    "depth = 2\n",
    "widths = [10, 10]\n",
    "lc_w_range = (0, 1.0)\n",
    "shift_w_range = (0., 2.0)\n",
    "\n",
    "phi = DiracPhi(depth, widths, lc_w_range, shift_w_range, device, tol = 1e-10).to(device)\n",
    "model = SurvivalCopula_nonlinear(phi, device = device, num_features=1, tol=1e-10).to(device)\n",
    "# optimizer = get_optim(optim_name, net, optim_args)\n",
    "\n",
    "optimizer_event = optim.Adam([{\"params\": [model.scale_t], \"lr\": 0.01},\n",
    "                            {\"params\": [model.shape_t], \"lr\": 0.01},\n",
    "                            {\"params\": model.net_t.parameters(), \"lr\": 0.01},\n",
    "                          ])\n",
    "optimizer_censoring = optim.Adam([{\"params\": [model.scale_c], \"lr\": 0.01},\n",
    "                            {\"params\": [model.shape_c], \"lr\": 0.01},\n",
    "                            {\"params\": model.net_c.parameters(), \"lr\": 0.01},\n",
    "                          ])\n",
    "optimizer_copula = optim.Adam([\n",
    "                            {\"params\": model.phi.parameters(), \"lr\": 0.01},\n",
    "                          ])\n",
    "\n",
    "train_loss_per_epoch = []\n",
    "print(\"Start training!\")\n",
    "for epoch in range(num_epochs):\n",
    "    loss_per_minibatch = []\n",
    "    for i, (x , t, c) in enumerate(train_loader, 0):\n",
    "        optimizer_copula.zero_grad()\n",
    "        optimizer_event.zero_grad()\n",
    "        optimizer_censoring.zero_grad()\n",
    "\n",
    "        p = model(x, t, c, max_iter = 10000)\n",
    "        logloss = -p\n",
    "        logloss.backward() \n",
    "        scalar_loss = (logloss/p.numel()).detach().cpu().numpy().item()\n",
    "\n",
    "        optimizer_censoring.step()\n",
    "        optimizer_event.step()\n",
    "        optimizer_copula.step()\n",
    "        \n",
    "        loss_per_minibatch.append(scalar_loss)\n",
    "    train_loss_per_epoch.append(np.mean(loss_per_minibatch))\n",
    "    if epoch % 100 == 0:\n",
    "        print('Training loss at epoch %s: %.5f' %\n",
    "                (epoch, train_loss_per_epoch[-1]))\n",
    "        print(f\"Shape Event: {model.shape_t.item(): .3f},\\\n",
    "            Shape Censoring: {model.shape_c.item(): .3f},\\\n",
    "            Scale Event: {model.scale_t.item(): .3f}, Scale Censoring: {model.scale_c.item(): .3f}\")\n",
    "    \n",
    "        val_loglikelihoods = []\n",
    "        for i, (x_val, t_val, c_val) in enumerate(val_loader, 0):\n",
    "            p_val = model(x_val, t_val, c_val, max_iter = 10000)\n",
    "            val_loglikelihood = p_val\n",
    "\n",
    "        print('Validation log-likelihood at epoch %s: %s' % (epoch, val_loglikelihood.cpu().detach().numpy().item()))\n",
    "    \n",
    "        # Check if validation loglikelihood has improved\n",
    "        if val_loglikelihood > best_val_loglikelihood:\n",
    "            best_val_loglikelihood = val_loglikelihood\n",
    "            epochs_no_improve = 0\n",
    "            torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'loss': best_val_loglikelihood,\n",
    "            }, 'checkpoint.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 100\n",
    "\n",
    "        # Early stopping condition\n",
    "        if epochs_no_improve == early_stop_epochs:\n",
    "            print('Early stopping triggered at epoch: %s' % epoch)\n",
    "            break\n",
    "    # Plot Samples from the learned copula\n",
    "    if epoch % 1000 == 0:\n",
    "        print('Scatter sampling')\n",
    "        samples = sample(model, 2, 10000, device =  device)\n",
    "        plt.scatter(samples[:, 0].cpu(), samples[:, 1].cpu())\n",
    "        plt.savefig('./sample_figs/epoch%s.png' %\n",
    "                    (epoch))\n",
    "        plt.clf()\n",
    "    \n",
    "    wandb.log({\"log_likelihood\": p, \n",
    "                \"event_shape\": model.shape_t.item(), \"censoring_shape\": model.shape_c.item(), \\\n",
    "                \"event_scale\": model.scale_t.item(), \"censoring_scale\": model.scale_c.item()})\n",
    "\n",
    "\n",
    "# Load the best model and plot samples\n",
    "checkpoint = torch.load('checkpoint.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "samples = sample(model, 2, 10000, device =  device)\n",
    "plt.scatter(samples[:, 0].cpu(), samples[:, 1].cpu())\n",
    "plt.savefig('./sample_figs/best_epoch.png')\n",
    "plt.clf()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter Plot\n",
    "checkpoint = torch.load('checkpoint.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "samples =  sample(model, 2, 3000, device =  device)\n",
    "plt.scatter(samples[:, 0].cpu(), samples[:, 1].cpu(), s = 15)\n",
    "plt.savefig('./sample_figs/best_epoch.png')\n",
    "plt.clf()\n",
    "\n",
    "# plotting with known copula\n",
    "known_copula = FrankCopula(10, k_dim=2)\n",
    "samples_known = known_copula.rvs(3000)\n",
    "plt.scatter(samples_known[:, 0], samples_known[:, 1], s = 15)\n",
    "\n",
    "plt.savefig('./sample_figs/best_true_sample.png')\n",
    "plt.clf()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joint Distribution (Cumulative Distribution Function) Spectral Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learned model\n",
    "n = 500\n",
    "x1 = np.linspace(0.001, 1, n)\n",
    "x2 = np.linspace(0.001, 1, n)\n",
    "xv1, xv2 = np.meshgrid(x1, x2)\n",
    "xv1_tensor = torch.tensor(xv1.flatten()).to(device)\n",
    "xv2_tensor = torch.tensor(xv2.flatten()).to(device)\n",
    "query = torch.stack((xv1_tensor, xv2_tensor)\n",
    "                    ).double().t().requires_grad_(True)\n",
    "cdf = model.cond_cdf(query, mode='cdf')\n",
    "pdf = model.cond_cdf(query, mode='pdf')\n",
    "\n",
    "heat_cdf = cdf.reshape(n,n).cpu().detach().numpy()\n",
    "heat_pdf = pdf.reshape(n,n).cpu().detach().numpy()\n",
    "\n",
    "# Create filled contour plot\n",
    "plt.contourf(x1, x2, heat_cdf, 200, cmap='Spectral_r')\n",
    "# Add a colorbar\n",
    "# plt.colorbar(label='Cumulative Distribution Function')\n",
    "# Show the plot\n",
    "plt.savefig('Frank_Learned_CDF.png', dpi=300)\n",
    "\n",
    "plt.clf()\n",
    "# Create filled contour plot\n",
    "plt.contourf(x1, x2, np.log(heat_pdf), 200, cmap='Spectral_r')\n",
    "# Add a colorbar\n",
    "# plt.colorbar(label='Log Density Function')\n",
    "# Show the plot\n",
    "plt.savefig('Frank_Learned_LogPDF.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Known copula\n",
    "from statsmodels.distributions.copula.api import (\n",
    "    CopulaDistribution, GumbelCopula, FrankCopula, ClaytonCopula)\n",
    "\n",
    "n = 1000\n",
    "x1 = np.linspace(0.001, 1, n)\n",
    "x2 = np.linspace(0.001, 1, n)\n",
    "xv1, xv2 = np.meshgrid(x1, x2)\n",
    "xv1_tensor = torch.tensor(xv1.flatten())\n",
    "xv2_tensor = torch.tensor(xv2.flatten())\n",
    "query = torch.stack((xv1_tensor, xv2_tensor)\n",
    "                    ).t().numpy()\n",
    "print(query.shape)\n",
    "\n",
    "known_copula = FrankCopula(10, k_dim=2)\n",
    "heat_cdf = known_copula.cdf(u=query).reshape(n,n)\n",
    "heat_pdf = known_copula.pdf(u=query).reshape(n,n)\n",
    "\n",
    "# Create filled contour plot\n",
    "plt.contourf(x1, x2, heat_cdf, 200, cmap='Spectral_r')\n",
    "plt.tick_params(axis='both', direction='in', length=4, width=1)\n",
    "# Add a colorbar\n",
    "# plt.colorbar(label='Cumulative Distribution Function')\n",
    "# Show the plot\n",
    "# plt.show()\n",
    "plt.savefig('Frank_Known_CDF.png', dpi=300)\n",
    "\n",
    "plt.clf()\n",
    "# Create filled contour plot\n",
    "plt.contourf(x1, x2, np.log(heat_pdf), 200, cmap='Spectral_r')\n",
    "plt.tick_params(axis='both', direction='in', length=4, width=1)\n",
    "# Add a colorbar\n",
    "# plt.colorbar(label='Log Density Function')\n",
    "# Show the plot\n",
    "# plt.show()\n",
    "plt.savefig('Frank_Known_LogPDF.png', dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
