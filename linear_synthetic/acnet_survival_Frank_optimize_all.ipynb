{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "seed = 142857\n",
    "rng = np.random.default_rng(seed)\n",
    "\n",
    "from statsmodels.distributions.copula.api import (\n",
    "    CopulaDistribution, GumbelCopula, FrankCopula, ClaytonCopula)\n",
    "\n",
    "# torch.set_num_threads(8)\n",
    "torch.backends.cudnn.allow_tf32 = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "# Generate synthetic data (time-to-event and censoring indicator)\n",
    "# np.random.seed(285714)\n",
    "v_e=4; rho_e=14; v_c=3; rho_c=16\n",
    "\n",
    "size = 30000\n",
    "dim = 10\n",
    "batch_size = 20000\n",
    "\n",
    "\n",
    "# generate X from 10 dimensional uniform distribution from 0 to 1\n",
    "# X = np.random.uniform(0, 1, (size, dim))\n",
    "X = rng.uniform(0, 1, (size, dim))\n",
    "# generate censoring risk coefficient beta from 10 dimensional uniforma distribution from 0 to 1\n",
    "# beta_c = np.random.uniform(0, 1, (dim, ))\n",
    "beta_c = rng.uniform(0, 1, (dim, ))\n",
    "# generate event risk coefficient beta_e from 10 dimensional uniforma distribution from 0 to 1\n",
    "# beta_e = np.random.uniform(0, 1, (dim,))\n",
    "beta_e = rng.uniform(0, 1, (dim,))\n",
    "# multiply beta_e with X to get event risk\n",
    "event_risk = np.matmul(X, beta_e).squeeze()\n",
    "# multiple beta_c with X to get censoring risk\n",
    "censoring_risk = np.matmul(X, beta_c).squeeze()\n",
    "\n",
    "\n",
    "copula = FrankCopula(theta=5)\n",
    "sample = copula.rvs(size,random_state=rng)\n",
    "u = sample[:, 0]\n",
    "v = sample[:, 1]\n",
    "plt.scatter(u,v)\n",
    "\n",
    "\n",
    "# Generate according to Algorithm 2 in \"Copula-based Deep Survival Models for Dependent Censoring\"\n",
    "def inverse_transform(value, risk, shape, scale):\n",
    "    return (-np.log(value)/np.exp(risk))**(1/shape)*scale       \n",
    "    # return (-np.log(1-value)/np.exp(risk))**(1/shape)*scale\n",
    "\n",
    "\n",
    "event_time = inverse_transform(u, event_risk, v_e, rho_e)\n",
    "censoring_time = inverse_transform(v, censoring_risk, v_c, rho_c)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(event_time, censoring_time)\n",
    "\n",
    "# check censoring rate \n",
    "print(np.sum(event_time<censoring_time)/len(event_time))\n",
    "\n",
    "# create observed time \n",
    "observed_time = np.minimum(event_time, censoring_time)\n",
    "event_indicator = (event_time<censoring_time).astype(int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Likelihood with ACNET copula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function, gradcheck\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os,sys\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "from main import sample\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "num_epochs = 500000\n",
    "\n",
    "times_tensor = torch.tensor(observed_time, dtype=torch.float64).to(device)\n",
    "event_indicator_tensor = torch.tensor(event_indicator, dtype=torch.float64).to(device)\n",
    "covariate_tensor = torch.tensor(X, dtype=torch.float64).to(device)\n",
    "train_data = TensorDataset(covariate_tensor[0:20000], times_tensor[0:20000], event_indicator_tensor[0:20000])\n",
    "val_data = TensorDataset(covariate_tensor[20000:], times_tensor[20000:], event_indicator_tensor[20000:])\n",
    "train_loader = DataLoader(train_data, batch_size= batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size= batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dirac_phi import DiracPhi\n",
    "from survival import SurvivalCopula\n",
    "from survival import sample\n",
    "\n",
    "\n",
    "# Early stopping\n",
    "best_val_loglikelihood = float('-inf')\n",
    "epochs_no_improve = 0\n",
    "early_stop_epochs = 2000\n",
    "\n",
    "# Parameters for ACNet\n",
    "depth = 2\n",
    "widths = [100, 100]\n",
    "lc_w_range = (0, 1.0)\n",
    "shift_w_range = (0., 2.0)\n",
    "\n",
    "phi = DiracPhi(depth, widths, lc_w_range, shift_w_range, device, tol = 1e-10).to(device)\n",
    "model = SurvivalCopula(phi, device = device, num_features=10, tol=1e-10).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "\n",
    "train_loss_per_epoch = []\n",
    "print(\"Start training!\")\n",
    "for epoch in range(num_epochs):\n",
    "    loss_per_minibatch = []\n",
    "    for i, (x , t, c) in enumerate(train_loader, 0):\n",
    "        # optimizer_copula.zero_grad()\n",
    "        # optimizer_event.zero_grad()\n",
    "        # optimizer_censoring.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        p = model(x, t, c, max_iter = 10000)\n",
    "        logloss = -p\n",
    "        logloss.backward() \n",
    "        scalar_loss = (logloss/p.numel()).detach().cpu().numpy().item()\n",
    "        \n",
    "        optimizer.step()\n",
    "        # optimizer_censoring.step()\n",
    "        # optimizer_event.step()\n",
    "        # optimizer_copula.step()\n",
    "        \n",
    "        loss_per_minibatch.append(scalar_loss)\n",
    "    train_loss_per_epoch.append(np.mean(loss_per_minibatch))\n",
    "    if epoch % 100 == 0:\n",
    "        print('Training loss at epoch %s: %.5f' %\n",
    "                (epoch, train_loss_per_epoch[-1]))\n",
    "        print(f\"Shape Event: {model.shape_t.item(): .3f},\\\n",
    "            Shape Censoring: {model.shape_c.item(): .3f},\\\n",
    "            Scale Event: {model.scale_t.item(): .3f}, Scale Censoring: {model.scale_c.item(): .3f}\")\n",
    "    \n",
    "        # val_loglikelihoods = []\n",
    "        for i, (x_val, t_val, c_val) in enumerate(val_loader, 0):\n",
    "            val_loglikelihood = model(x_val, t_val, c_val, max_iter = 10000)\n",
    "\n",
    "        print('Validation log-likelihood at epoch %s: %s' % (epoch, val_loglikelihood.cpu().detach().numpy().item()))\n",
    "    \n",
    "        # Check if validation loglikelihood has improved\n",
    "        if val_loglikelihood > best_val_loglikelihood:\n",
    "            best_val_loglikelihood = val_loglikelihood\n",
    "            epochs_no_improve = 0\n",
    "            torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'loss': best_val_loglikelihood,\n",
    "            }, 'checkpoint.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 100\n",
    "\n",
    "        # Early stopping condition\n",
    "        if epochs_no_improve == early_stop_epochs:\n",
    "            print('Early stopping triggered at epoch: %s' % epoch)\n",
    "            break\n",
    "    # Plot Samples from the learned copula\n",
    "    if epoch % 1000 == 0:\n",
    "        print('Scatter sampling')\n",
    "        samples = sample(model, 2, 10000, device =  device)\n",
    "        plt.scatter(samples[:, 0].cpu(), samples[:, 1].cpu())\n",
    "        plt.savefig('./sample_figs/epoch%s.png' %\n",
    "                    (epoch))\n",
    "        plt.clf()\n",
    "\n",
    "\n",
    "# Load the best model and plot samples\n",
    "checkpoint = torch.load('checkpoint.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "samples = sample(model, 2, 10000, device =  device)\n",
    "plt.scatter(samples[:, 0].cpu(), samples[:, 1].cpu())\n",
    "plt.savefig('./sample_figs/best_epoch.png')\n",
    "plt.clf()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter Plot\n",
    "checkpoint = torch.load('checkpoint.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "samples =  sample(model, 2, 3000, device =  device)\n",
    "plt.scatter(samples[:, 0].cpu(), samples[:, 1].cpu(), s = 15)\n",
    "plt.savefig('./sample_figs/frank/best_epoch.png')\n",
    "plt.clf()\n",
    "\n",
    "# plotting with known copula\n",
    "known_copula = FrankCopula(10, k_dim=2)\n",
    "samples_known = known_copula.rvs(3000)\n",
    "plt.scatter(samples_known[:, 0], samples_known[:, 1], s = 15)\n",
    "\n",
    "plt.savefig('./sample_figs/frank/best_true_sample.png')\n",
    "plt.clf()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joint Distribution (Cumulative Distribution Function) Spectral Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learned model\n",
    "n = 500\n",
    "x1 = np.linspace(0.001, 1, n)\n",
    "x2 = np.linspace(0.001, 1, n)\n",
    "xv1, xv2 = np.meshgrid(x1, x2)\n",
    "xv1_tensor = torch.tensor(xv1.flatten()).to(device)\n",
    "xv2_tensor = torch.tensor(xv2.flatten()).to(device)\n",
    "query = torch.stack((xv1_tensor, xv2_tensor)\n",
    "                    ).double().t().requires_grad_(True)\n",
    "cdf = model.cond_cdf(query, mode='cdf')\n",
    "pdf = model.cond_cdf(query, mode='pdf')\n",
    "\n",
    "heat_cdf = cdf.reshape(n,n).cpu().detach().numpy()\n",
    "heat_pdf = pdf.reshape(n,n).cpu().detach().numpy()\n",
    "\n",
    "# Create filled contour plot\n",
    "plt.contourf(x1, x2, heat_cdf, 200, cmap='Spectral_r')\n",
    "# Add a colorbar\n",
    "# plt.colorbar(label='Cumulative Distribution Function')\n",
    "# Show the plot\n",
    "plt.savefig('Frank_Learned_CDF.png', dpi=300)\n",
    "\n",
    "plt.clf()\n",
    "# Create filled contour plot\n",
    "plt.contourf(x1, x2, np.log(heat_pdf), 200, cmap='Spectral_r')\n",
    "# Add a colorbar\n",
    "# plt.colorbar(label='Log Density Function')\n",
    "# Show the plot\n",
    "plt.savefig('Frank_Learned_LogPDF.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Known copula\n",
    "from statsmodels.distributions.copula.api import (\n",
    "    CopulaDistribution, GumbelCopula, FrankCopula, ClaytonCopula)\n",
    "\n",
    "n = 1000\n",
    "x1 = np.linspace(0.001, 1, n)\n",
    "x2 = np.linspace(0.001, 1, n)\n",
    "xv1, xv2 = np.meshgrid(x1, x2)\n",
    "xv1_tensor = torch.tensor(xv1.flatten())\n",
    "xv2_tensor = torch.tensor(xv2.flatten())\n",
    "query = torch.stack((xv1_tensor, xv2_tensor)\n",
    "                    ).t().numpy()\n",
    "print(query.shape)\n",
    "\n",
    "known_copula = FrankCopula(10, k_dim=2)\n",
    "heat_cdf = known_copula.cdf(u=query).reshape(n,n)\n",
    "heat_pdf = known_copula.pdf(u=query).reshape(n,n)\n",
    "\n",
    "# Create filled contour plot\n",
    "plt.contourf(x1, x2, heat_cdf, 200, cmap='Spectral_r')\n",
    "plt.tick_params(axis='both', direction='in', length=4, width=1)\n",
    "# Add a colorbar\n",
    "# plt.colorbar(label='Cumulative Distribution Function')\n",
    "# Show the plot\n",
    "# plt.show()\n",
    "plt.savefig('Frank_Known_CDF.png', dpi=300)\n",
    "\n",
    "plt.clf()\n",
    "# Create filled contour plot\n",
    "plt.contourf(x1, x2, np.log(heat_pdf), 200, cmap='Spectral_r')\n",
    "plt.tick_params(axis='both', direction='in', length=4, width=1)\n",
    "# Add a colorbar\n",
    "# plt.colorbar(label='Log Density Function')\n",
    "# Show the plot\n",
    "# plt.show()\n",
    "plt.savefig('Frank_Known_LogPDF.png', dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
